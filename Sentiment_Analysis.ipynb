{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5D4m3Hk/eXnHl95zEIk5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnganaB/Sentiment-Analysis-/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSBYWSNs39Lx",
        "colab_type": "code",
        "outputId": "55a8a488-139e-4136-87e2-b265c5b4c3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "source": [
        "!pip install tf-nightly\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/07/38614753fe6e1efa4ce0a5b8a1fdedf5e5adc486f7a096ad16ee09e13b83/tf_nightly-2.3.0.dev20200515-cp36-cp36m-manylinux2010_x86_64.whl (522.4MB)\n",
            "\u001b[K     |████████████████████████████████| 522.4MB 29kB/s \n",
            "\u001b[?25hCollecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/39/a3/908e96589ab5bbe7feee06730a02d4de77664053fec34764a93beab95b4b/tb_nightly-2.3.0a20200515-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.28.1)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/79/e6357c23fbdde895654fdc6e4d0a916dd92da4fb4b1548d0b72dec5b01d7/tf_estimator_nightly-2.3.0.dev2020051501-py2.py3-none-any.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 49.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Collecting keras-preprocessing<1.2,>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (46.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, keras-preprocessing, tf-nightly\n",
            "  Found existing installation: Keras-Preprocessing 1.1.0\n",
            "    Uninstalling Keras-Preprocessing-1.1.0:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
            "Successfully installed keras-preprocessing-1.1.2 tb-nightly-2.3.0a20200515 tf-estimator-nightly-2.3.0.dev2020051501 tf-nightly-2.3.0.dev20200515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJfCGGodFVGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers \n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMsQB-LZGIsa",
        "colab_type": "code",
        "outputId": "72c57457-b53d-424d-943f-a5f64955284e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "embedding_layer = layers.Embedding(1000, 5)\n",
        "result = embedding_layer(tf.constant([[0,1,2],[3,4,5]]))\n",
        "print(result.shape)\n",
        "result.numpy()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 0.02698601,  0.02004528,  0.01985308,  0.01964689,\n",
              "         -0.04679755],\n",
              "        [-0.02021384, -0.01363242, -0.02185481, -0.01683718,\n",
              "         -0.01360739],\n",
              "        [ 0.02763114,  0.00622646, -0.00568942, -0.02468425,\n",
              "         -0.00262558]],\n",
              "\n",
              "       [[ 0.04031687,  0.00602959,  0.03000039,  0.04100487,\n",
              "          0.00411282],\n",
              "        [ 0.01987252, -0.04768006,  0.01122787, -0.01153723,\n",
              "          0.03862769],\n",
              "        [ 0.04800132,  0.01668774, -0.00170603, -0.0330503 ,\n",
              "         -0.00612079]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1qDS1BqIgS3",
        "colab_type": "code",
        "outputId": "f629c89d-3d7e-4091-ca10-18bbead739cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "(train_data, test_data), info = tfds.load(\n",
        "    'imdb_reviews/subwords8k', \n",
        "    split = (tfds.Split.TRAIN, tfds.Split.TEST), \n",
        "    with_info=True, as_supervised=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews/subwords8k/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0...\u001b[0m\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incompleteR57MZC/imdb_reviews-train.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incompleteR57MZC/imdb_reviews-test.tfrecord\n",
            "Shuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incompleteR57MZC/imdb_reviews-unsupervised.tfrecord\n",
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWwPFk3BUzhH",
        "colab_type": "code",
        "outputId": "8ed10707-3cb4-41e6-a346-fee16af71308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = info.features['text'].encoder # \"_\" represents spaces in the vocabulary\n",
        "print('Vocabulary size: {}'.format(encoder.vocab_size))\n",
        "encoder.subwords[:10]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 8185\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the_', ', ', '. ', 'a_', 'and_', 'of_', 'to_', 's_', 'is_', 'br']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVJpXmqJtXAf",
        "colab_type": "code",
        "outputId": "f7e02b81-2cd5-4252-c94a-b961da63131d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_string = 'Hello TensorFlow.'\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print('The original string: \"{}\"'.format(original_string))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded string is [4025, 222, 6307, 2327, 4043, 2120, 7975]\n",
            "The original string: \"Hello TensorFlow.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU0tPnnvtmOB",
        "colab_type": "code",
        "outputId": "0714c879-698a-4610-9e5a-c8b339374474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "for index in encoded_string:\n",
        "  print('{} ----> {}'.format(index, encoder.decode([index])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4025 ----> Hell\n",
            "222 ----> o \n",
            "6307 ----> Ten\n",
            "2327 ----> sor\n",
            "4043 ----> Fl\n",
            "2120 ----> ow\n",
            "7975 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOSnZsQBTXm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used to standardize the lengths of reviews\n",
        "train_batches = train_data.shuffle(10000).padded_batch(64)\n",
        "test_batches = test_data.shuffle(10000).padded_batch(64) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mW2dr7MIXYvS",
        "colab_type": "code",
        "outputId": "e51263f9-d775-402d-8e1b-97960e08dac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "train_batch, train_labels = next(iter(train_batches))\n",
        "train_batch.numpy()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  12, 7422, 2054, ...,    0,    0,    0],\n",
              "       [2882,  160,  270, ...,    0,    0,    0],\n",
              "       [ 394, 4136, 7961, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [  19,   98,    5, ...,    0,    0,    0],\n",
              "       [  62,   27,  180, ...,    0,    0,    0],\n",
              "       [  62,  435,    9, ...,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHQfpNm8XuhZ",
        "colab_type": "code",
        "outputId": "c22de4ee-2ace-4517-85ea-4c5b31e868cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "embedding_dim=64\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim, mask_zero = True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 64)          523840    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 128)         66048     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                41216     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 635,329\n",
            "Trainable params: 635,329\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuM8EphJXyZb",
        "colab_type": "code",
        "outputId": "23115f1b-b842-422e-fb16-5bb3e9b0a7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_batches, epochs=10,\n",
        "                    validation_data=test_batches,\n",
        "                    validation_steps=30)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "391/391 [==============================] - 1657s 4s/step - loss: 0.6330 - accuracy: 0.5902 - val_loss: 0.4437 - val_accuracy: 0.8083\n",
            "Epoch 2/10\n",
            "391/391 [==============================] - 1696s 4s/step - loss: 0.3576 - accuracy: 0.8536 - val_loss: 0.3318 - val_accuracy: 0.8474\n",
            "Epoch 3/10\n",
            "391/391 [==============================] - 1705s 4s/step - loss: 0.2514 - accuracy: 0.9091 - val_loss: 0.3152 - val_accuracy: 0.8641\n",
            "Epoch 4/10\n",
            "391/391 [==============================] - 1648s 4s/step - loss: 0.2062 - accuracy: 0.9285 - val_loss: 0.3699 - val_accuracy: 0.8609\n",
            "Epoch 5/10\n",
            "391/391 [==============================] - 1634s 4s/step - loss: 0.1724 - accuracy: 0.9437 - val_loss: 0.3267 - val_accuracy: 0.8703\n",
            "Epoch 6/10\n",
            "391/391 [==============================] - 1628s 4s/step - loss: 0.1467 - accuracy: 0.9547 - val_loss: 0.3417 - val_accuracy: 0.8719\n",
            "Epoch 7/10\n",
            "391/391 [==============================] - 1681s 4s/step - loss: 0.1251 - accuracy: 0.9639 - val_loss: 0.3941 - val_accuracy: 0.8578\n",
            "Epoch 8/10\n",
            "391/391 [==============================] - 1628s 4s/step - loss: 0.1088 - accuracy: 0.9712 - val_loss: 0.4211 - val_accuracy: 0.8641\n",
            "Epoch 9/10\n",
            "391/391 [==============================] - 1599s 4s/step - loss: 0.0949 - accuracy: 0.9750 - val_loss: 0.5149 - val_accuracy: 0.8458\n",
            "Epoch 10/10\n",
            "391/391 [==============================] - 1672s 4s/step - loss: 0.0935 - accuracy: 0.9746 - val_loss: 0.4788 - val_accuracy: 0.8510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rpH_SKqxNXv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e613e564-31a7-4e30-be86-164fcdb1258a"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_batches)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 353s 902ms/step - loss: 0.4830 - accuracy: 0.8517\n",
            "Test Loss: 0.4830092191696167\n",
            "Test Accuracy: 0.8517199754714966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prNnDB-QxTYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_to_size(vec, size):\n",
        "  zeros = [0] * (size - len(vec))\n",
        "  vec.extend(zeros)\n",
        "  return vec\n",
        "\n",
        "def sample_predict(sample_pred_text, pad):\n",
        "  encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
        "\n",
        "  if pad:\n",
        "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
        "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
        "  predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
        "\n",
        "  return (predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCbJCrKtL7tL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65ddc598-a9ed-4a23-cfdd-b788d68e285e"
      },
      "source": [
        "sample_pred_text = ('The movie was good. The animation and the graphics '\n",
        "                    'were awesome. I would definitely recommend this movie.')\n",
        "predictions = sample_predict(sample_pred_text, pad=True)\n",
        "print(predictions)  # if prediction > 0.5, positive review"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.8087221]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}